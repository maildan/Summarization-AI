# AI
maildan_AI

## Maildan\_kobart\_v3 ëª¨ë¸ ì¹´ë“œ

**Maildan\_kobart\_v3**ëŠ” í•œêµ­ì–´ í…ìŠ¤íŠ¸ ìš”ì•½ ë° ìì—°ìŠ¤ëŸ¬ìš´ ì´ì–´ì“°ê¸° ìƒì„±ì„ ëª©ì ìœ¼ë¡œ ê°œë°œëœ ëª¨ë¸ì…ë‹ˆë‹¤. ë³¸ ëª¨ë¸ì€ í•œêµ­ì–´ì— íŠ¹í™”ëœ ì‚¬ì „í•™ìŠµ ëª¨ë¸ **KoBART**(`gogamza/kobart-base-v2`)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ seq2seq ë°©ì‹ìœ¼ë¡œ íŒŒì¸íŠœë‹ ë˜ì—ˆìŠµë‹ˆë‹¤.

---

## ğŸ§  ëª¨ë¸ ê°œìš”

* **ëª¨ë¸ íƒ€ì…**: Transformer ê¸°ë°˜ Encoder-Decoder (KoBART)
* **ì‚¬ìš© ëª©ì **: ê¸°ì‚¬, ë‰´ìŠ¤, íšŒì˜ë¡, ì´ë©”ì¼ ë“±ì˜ í•œêµ­ì–´ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ê±°ë‚˜ ì„œì‚¬ì ìœ¼ë¡œ ì´ì–´ì“°ê¸°
* **íŒŒì¸íŠœë‹ ë°©ì‹**: í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ í•™ìŠµ (Prompt + ë¬¸ë§¥ ì…ë ¥ â†’ ì´ì–´ì“°ê¸°/ìš”ì•½ ì¶œë ¥)
* **ê¸°ë°˜ ëª¨ë¸**: [gogamza/kobart-base-v2](https://huggingface.co/gogamza/kobart-base-v2)
* **ì¶œë ¥ í˜•íƒœ**: ë°°ê²½, ì›ì¸, ìŸì  ë“± ë¬¸ë§¥ì„ ë°˜ì˜í•œ ìì—°ìŠ¤ëŸ¬ìš´ ìš”ì•½ ë˜ëŠ” ì´ì–´ì“°ê¸° ë¬¸ì¥

---

## ğŸ” ëª¨ë¸ ì£¼ìš” ê¸°ëŠ¥ (Top 3 Features)

1. **ì„œì‚¬ì ì´ê³  ë…¼ë¦¬ì ì¸ ìš”ì•½ ìƒì„±**: ë‹¨ìˆœ ì¶•ì•½ì´ ì•„ë‹Œ, ë°°ê²½-ì›ì¸-ê²°ê³¼ì˜ íë¦„ì„ ë‹´ì€ ë§¥ë½ ì¤‘ì‹¬ ìš”ì•½ ìƒì„±
2. **í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë§ì¶¤ ì‘ë‹µ**: ì…ë ¥ëœ ì§€ì‹œë¬¸(prompt)ì— ë”°ë¼ ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼, ì–´ì¡°, ì •ë³´ ë°€ë„ ë°˜ì˜ ê°€ëŠ¥
3. **ê²½ëŸ‰ êµ¬ì¡°ë¡œ ë¹ ë¥¸ ì¶”ë¡ **: ìƒëŒ€ì ìœ¼ë¡œ ì‘ì€ íŒŒë¼ë¯¸í„° ìˆ˜ë¡œ ë¡œì»¬ í™˜ê²½ì—ì„œë„ íš¨ìœ¨ì ì¸ ì¶”ë¡  ì„±ëŠ¥ ì œê³µ

> ğŸ“Œ ì‚¬ìš© ë²”ìœ„: í•œêµ­ì–´ ë‰´ìŠ¤, ì´ë©”ì¼, íšŒì˜ë¡ ë“±ì˜ ë¬¸ì„œë¥¼ ë…¼ë¦¬ì ì´ê³  í’ë¶€í•œ ìš”ì•½ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ë° í™œìš©ë©ë‹ˆë‹¤.

---

## ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡° ë° ì•„í‚¤í…ì²˜

```
ì…ë ¥ ë¬¸ì¥ + í”„ë¡¬í”„íŠ¸
        â”‚
        â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
|       KoBART ëª¨ë¸ êµ¬ì¡°       |
|   - ì¸ì½”ë”: ë¬¸ì¥ ë²¡í„°í™”       |
|   - ë””ì½”ë”: ì´ì–´ì“°ê¸°/ìš”ì•½ ìƒì„± |
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â”‚
        â–¼
 ì¶œë ¥ ë¬¸ì¥ (í’ë¶€í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ìš”ì•½ë¬¸)
```

* **KoBART**ëŠ” í•œêµ­ì–´ BART êµ¬ì¡°ë¡œ, ë¬¸ë§¥ì„ ì–‘ë°©í–¥ìœ¼ë¡œ ì´í•´í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ë° ê°•ì ì„ ê°€ì§‘ë‹ˆë‹¤.
* Seq2Seq êµ¬ì¡° ë•ë¶„ì—, ë¬¸ì¥ì„ ì¡°ê±´ë¶€ë¡œ ìƒì„±í•  ìˆ˜ ìˆì–´ ìš”ì•½, ë²ˆì—­, ì´ì–´ì“°ê¸°ì— ìµœì ì…ë‹ˆë‹¤.

---

## ğŸ§ª íŒŒì¸íŠœë‹ ì½”ë“œ ì˜ˆì‹œ

```python
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments

model_name = "EbanLee/kobart-summary-v3"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

dataset = load_dataset("json", data_files={"train": "path/to/train_summarized_1000.jsonl"})

prompt = (
    "ë‹¤ìŒ ê¸°ì‚¬ ë‚´ìš©ì„ ë‹¨ìˆœí•˜ê²Œ ìš”ì•½í•˜ì§€ ë§ê³ , ì„œì‚¬ êµ¬ì¡°ì™€ ë§¥ë½ì„ ì‚´ë ¤ 3~5ë¬¸ì¥ìœ¼ë¡œ í’ë¶€í•˜ê²Œ ìš”ì•½í•´ì¤˜. "
    "ì´ìŠˆê°€ ë°œìƒí•œ ë°°ê²½ê³¼ ì›ì¸, ì°¸ì—¬ìë“¤ì´ ì£¼ì¥í•˜ëŠ” í•µì‹¬ ë‚´ìš©ì´ ì£¼ì¥ì— ë‹´ê¸´ ì‚¬íšŒì  ì˜ë¯¸ë‚˜ ìŸì ì´ í¬í•¨ë˜ë„ë¡ ì‘ì„±í•´ì¤˜. "
    "ê°ì •ì ì´ì§€ ì•Šê³  ê°ê´€ì ì¸ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ë˜, ë…¼ìŸì˜ êµ¬ì¡°ëŠ” ë“œëŸ¬ë‚˜ê²Œ ì¨ì¤˜.\n\n"
)

def preprocess_function(examples):
    inputs = [prompt + "ë¬¸ì¥: " + text + "\n\nê¸€:" for text in examples["input"]]
    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding="max_length")
    labels = tokenizer(examples["output"], max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset["train"].map(preprocess_function, batched=True, remove_columns=["input", "output"])

training_args = TrainingArguments(
    output_dir="./kobart_prompt_tuned2",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=3e-2,
    logging_steps=100,
    save_steps=500,
    fp16=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

trainer.train()
```

---

## ğŸ“Œ ì¶”ë¡  ì½”ë“œ ì˜ˆì‹œ

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

model_name = "hienchong/Maildan_kobart_v3"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to("cuda" if torch.cuda.is_available() else "cpu")

prompt = (
    "ë‹¤ìŒ ê¸°ì‚¬ ë‚´ìš©ì„ ë‹¨ìˆœí•˜ê²Œ ìš”ì•½í•˜ì§€ ë§ê³ , ì„œì‚¬ êµ¬ì¡°ì™€ ë§¥ë½ì„ ì‚´ë ¤ 3~5ë¬¸ì¥ìœ¼ë¡œ í’ë¶€í•˜ê²Œ ìš”ì•½í•´ì¤˜. "
    "ì´ìŠˆì˜ ë°°ê²½, ì›ì¸, ì£¼ì¥ ë‚´ìš©, ì‚¬íšŒì  ì˜ë¯¸ë¥¼ í¬í•¨í•´ì¤˜. ê°ì •ì ì´ì§€ ì•Šê³  ê°ê´€ì ìœ¼ë¡œ ì„œìˆ í•´ì¤˜.\n\n"
)

text = (
    "ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€ëŠ” ê²½ë³µê¶ ë³µì› ì‚¬ì—…ì˜ ì¼í™˜ìœ¼ë¡œ ê´‘í™”ë¬¸ í˜„íŒì„ í•œê¸€ë¡œ êµì²´í•˜ìëŠ” ë…¼ì˜ë¥¼ ì‹œì‘í–ˆë‹¤. "
    "í•´ë‹¹ ì œì•ˆì€ ìœ ì¸ì´Œ ì¥ê´€ì˜ ìš”ì²­ì—ì„œ ë¹„ë¡¯ë˜ì—ˆìœ¼ë©°, í˜„ì¬ê¹Œì§€ëŠ” í•œìë¡œ ëœ í˜„íŒì´ ì‚¬ìš©ë˜ê³  ìˆë‹¤..."
)

input_text = prompt + "ë¬¸ì¥: " + text + "\n\nê¸€:"
inputs = tokenizer(input_text, return_tensors="pt", max_length=1024, truncation=True).to(model.device)

summary_ids = model.generate(**inputs, max_new_tokens=200, num_beams=4)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print(summary)
```

---

## ğŸ”“ ë¼ì´ì„ ìŠ¤

* ê¸°ë°˜ ëª¨ë¸: Apache 2.0 License
* í•™ìŠµ ë°ì´í„°: ë¹„ê³µê°œ / ì—°êµ¬ ë° ë¹„ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥

---

## ğŸ§­ í–¥í›„ ê³„íš

* ROUGE, BLEU ê¸°ë°˜ ì •ëŸ‰ ì„±ëŠ¥ ìˆ˜ì¹˜ ì¶”ê°€ ì˜ˆì •
* Gradio/Streamlit ê¸°ë°˜ ë°ëª¨ í˜ì´ì§€ ê°œë°œ ì˜ˆì •
* ë‹¤ì–‘í•œ Task (ë©”ì¼ ì‘ë‹µ ìƒì„±, ë‰´ìŠ¤ í•´ì„¤ ë“±)ë¡œ ì„¸ë¶„í™”ëœ ì‘ìš© í™•ëŒ€

---

## ğŸ™‹â€â™€ï¸ ì œì‘ì ì •ë³´

* ì´ë¦„: ë¥˜í˜„ì • (Hienchong)
* Hugging Face: [https://huggingface.co/hienchong](https://huggingface.co/hienchong)
* ëª¨ë¸ í˜ì´ì§€: [https://huggingface.co/hienchong/Maildan\_kobart\_v3](https://huggingface.co/hienchong/Maildan_kobart_v3)
