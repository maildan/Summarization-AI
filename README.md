# ğŸ¤– Maildan_AI

## ğŸ“˜ Maildan_kobart_v3 ëª¨ë¸ ì¹´ë“œ

`Maildan_kobart_v3`ëŠ” í•œêµ­ì–´ ë¬¸ì„œì˜ **ì„œì‚¬ì  ìš”ì•½** ë° **ìì—°ìŠ¤ëŸ¬ìš´ ì´ì–´ì“°ê¸°**ë¥¼ ëª©í‘œë¡œ íŒŒì¸íŠœë‹ëœ KoBART ê¸°ë°˜ Transformer ëª¨ë¸ì…ë‹ˆë‹¤.  
í”„ë¡¬í”„íŠ¸ íŠœë‹ ê¸°ë°˜ì˜ í•™ìŠµ ë°©ì‹ì„ í™œìš©í•´ ë¬¸ë§¥ì„ ì´í•´í•˜ê³  í’ë¶€í•œ ìš”ì•½ë¬¸ì„ ìƒì„±í•˜ëŠ” ë° ê°•ì ì„ ë³´ì…ë‹ˆë‹¤.



## ğŸ§  ëª¨ë¸ ê°œìš”

| í•­ëª© | ì„¤ëª… |
|------|------|
| **ëª¨ë¸ íƒ€ì…** | Transformer ê¸°ë°˜ Seq2Seq (KoBART) |
| **í•™ìŠµ ëª©ì ** | ë‰´ìŠ¤, ê¸°ì‚¬, íšŒì˜ë¡, ì´ë©”ì¼ ë“±ì˜ í•œêµ­ì–´ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ê±°ë‚˜ ì´ì–´ì“°ê¸° |
| **íŠœë‹ ë°©ì‹** | í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ íŒŒì¸íŠœë‹ (Prompt + ë¬¸ë§¥ ì…ë ¥ â†’ ì´ì–´ì“°ê¸°/ìš”ì•½ ì¶œë ¥) |
| **ê¸°ë°˜ ëª¨ë¸** | [`EbanLee/kobart-summary-v3`](https://huggingface.co/EbanLee/kobart-summary-v3) |
| **ì¶œë ¥ í˜•íƒœ** | ë°°ê²½, ì›ì¸, ìŸì  ë“±ì„ ë°˜ì˜í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì„œì‚¬í˜• ìš”ì•½ ë¬¸ì¥ |



## âœ¨ ì£¼ìš” ê¸°ëŠ¥

1. **ì„œì‚¬ì  ìš”ì•½ ìƒì„±**  
   ë‹¨ìˆœ ìš”ì•½ì´ ì•„ë‹Œ, ë…¼ë¦¬ êµ¬ì¡°ì™€ ì˜ë¯¸ íë¦„ì„ ë‹´ì€ 3~5ë¬¸ì¥ ìš”ì•½ ìƒì„±

2. **í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì œì–´**  
   ì§€ì‹œë¬¸(prompt)ì„ í†µí•´ ë¬¸ì²´, ì •ë³´ ë°€ë„, ì–´ì¡°ë¥¼ ìœ ì—°í•˜ê²Œ ì œì–´ ê°€ëŠ¥

3. **ê²½ëŸ‰ ì¶”ë¡  ìµœì í™”**  
   ìƒëŒ€ì ìœ¼ë¡œ ì ì€ íŒŒë¼ë¯¸í„°ë¡œë„ ë¡œì»¬ì—ì„œ ë¹ ë¥´ê³  ì•ˆì •ì ì¸ ì¶”ë¡  ê°€ëŠ¥

> ğŸ“Œ **í™œìš© ì˜ˆì‹œ**: ë‰´ìŠ¤, ì¹¼ëŸ¼, íšŒì˜ë¡ ë“± ì‹¤ìš© í…ìŠ¤íŠ¸ë¥¼ ë…¼ë¦¬ì ì´ë©° ì••ì¶•ë ¥ ìˆê²Œ ì¬êµ¬ì„±



## ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡° ë° ì•„í‚¤í…ì²˜

ì…ë ¥ ë¬¸ì¥ + í”„ë¡¬í”„íŠ¸
â†“
[ KoBART Seq2Seq ëª¨ë¸ ]
â†“
ì¶œë ¥ ë¬¸ì¥ (í’ë¶€í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ìš”ì•½/ì´ì–´ì“°ê¸° ê²°ê³¼)

python
ë³µì‚¬
í¸ì§‘

- **ì¸ì½”ë”**: ì…ë ¥ ë¬¸ì¥ì„ ë²¡í„°ë¡œ ì¸ì½”ë”©  
- **ë””ì½”ë”**: ë¬¸ë§¥ ê¸°ë°˜ìœ¼ë¡œ ì´ì–´ì§€ëŠ” ë¬¸ì¥ ìƒì„±  



## ğŸ§ª íŒŒì¸íŠœë‹ ì½”ë“œ ì˜ˆì‹œ

```python
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments

model_name = "EbanLee/kobart-summary-v3"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

dataset = load_dataset("json", data_files={"train": "path/to/train_summarized_1000.jsonl"})

prompt = (
    "ë‹¤ìŒ ê¸°ì‚¬ ë‚´ìš©ì„ ë‹¨ìˆœí•˜ê²Œ ìš”ì•½í•˜ì§€ ë§ê³ , ì„œì‚¬ êµ¬ì¡°ì™€ ë§¥ë½ì„ ì‚´ë ¤ 3~5ë¬¸ì¥ìœ¼ë¡œ í’ë¶€í•˜ê²Œ ìš”ì•½í•´ì¤˜. "
    "ì´ìŠˆì˜ ë°°ê²½ê³¼ ì›ì¸, ì°¸ì—¬ìë“¤ì˜ ì£¼ì¥, ì‚¬íšŒì  ì˜ë¯¸ì™€ ìŸì ì„ í¬í•¨í•´ì¤˜. ê°ê´€ì ì¸ ì–´ì¡°ë¡œ ì„œìˆ í•´ì¤˜.\n\n"
)

def preprocess_function(examples):
    inputs = [prompt + "ë¬¸ì¥: " + text + "\n\nê¸€:" for text in examples["input"]]
    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding="max_length")
    labels = tokenizer(examples["output"], max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset["train"].map(preprocess_function, batched=True, remove_columns=["input", "output"])

training_args = TrainingArguments(
    output_dir="./kobart_prompt_tuned2",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=3e-2,
    logging_steps=100,
    save_steps=500,
    fp16=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

trainer.train()
ğŸš€ ì¶”ë¡  ì½”ë“œ ì˜ˆì‹œ
python
ë³µì‚¬
í¸ì§‘
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

model_name = "hienchong/Maildan_kobart_v3"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to("cuda" if torch.cuda.is_available() else "cpu")

prompt = (
    "ë‹¤ìŒ ê¸°ì‚¬ ë‚´ìš©ì„ ë‹¨ìˆœí•˜ê²Œ ìš”ì•½í•˜ì§€ ë§ê³ , ì„œì‚¬ êµ¬ì¡°ì™€ ë§¥ë½ì„ ì‚´ë ¤ 3~5ë¬¸ì¥ìœ¼ë¡œ í’ë¶€í•˜ê²Œ ìš”ì•½í•´ì¤˜. "
    "ì´ìŠˆì˜ ë°°ê²½, ì›ì¸, ì£¼ì¥ ë‚´ìš©, ì‚¬íšŒì  ì˜ë¯¸ë¥¼ í¬í•¨í•´ì¤˜. ê°ì •ì ì´ì§€ ì•Šê³  ê°ê´€ì ìœ¼ë¡œ ì„œìˆ í•´ì¤˜.\n\n"
)

text = (
    "ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€ëŠ” ê²½ë³µê¶ ë³µì› ì‚¬ì—…ì˜ ì¼í™˜ìœ¼ë¡œ ê´‘í™”ë¬¸ í˜„íŒì„ í•œê¸€ë¡œ êµì²´í•˜ìëŠ” ë…¼ì˜ë¥¼ ì‹œì‘í–ˆë‹¤. "
    "í•´ë‹¹ ì œì•ˆì€ ìœ ì¸ì´Œ ì¥ê´€ì˜ ìš”ì²­ì—ì„œ ë¹„ë¡¯ë˜ì—ˆìœ¼ë©°, í˜„ì¬ê¹Œì§€ëŠ” í•œìë¡œ ëœ í˜„íŒì´ ì‚¬ìš©ë˜ê³  ìˆë‹¤..."
)

input_text = prompt + "ë¬¸ì¥: " + text + "\n\nê¸€:"
inputs = tokenizer(input_text, return_tensors="pt", max_length=1024, truncation=True).to(model.device)

summary_ids = model.generate(**inputs, max_new_tokens=200, num_beams=4)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print(summary)
```

## ğŸ› ï¸ ì„¤ì¹˜ ëª¨ë“ˆ ë° ì—­í• 

```python
bash
pip install torch transformers accelerate datasets peft
```

| ëª¨ë“ˆ                 | ì„¤ëª…                                                  |
| ------------------ | --------------------------------------------------- |
| **`torch`**        | PyTorch ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¡œ, ëª¨ë¸ í•™ìŠµ/ì¶”ë¡ ì˜ í•µì‹¬ ì—­í•  ìˆ˜í–‰              |
| **`transformers`** | Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” ëª¨ë¸, í† í¬ë‚˜ì´ì €, `Trainer` ë“± í•µì‹¬ ë„êµ¬ ëª¨ìŒ |
| **`accelerate`**   | ë‹¤ì–‘í•œ í™˜ê²½(CPU, GPU, TPU ë“±)ì—ì„œ ì†ì‰¬ìš´ ë¶„ì‚° í•™ìŠµ ë° ì‹¤í–‰ ì§€ì›         |
| **`datasets`**     | JSONL í¬í•¨ ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë°ì´í„°ì…‹ ë¡œë”© ë° ì „ì²˜ë¦¬ ê¸°ëŠ¥ ì œê³µ                |
| **`peft`**         | PEFT(í”„ë¡¬í”„íŠ¸ íŠœë‹, LoRA ë“±)ë¥¼ í†µí•´ ê²½ëŸ‰í™”ëœ íŒŒì¸íŠœë‹ êµ¬í˜„ ê°€ëŠ¥           |


```python
bash
pip install peft
```

## ğŸ“„ ë¼ì´ì„ ìŠ¤

- ê¸°ë°˜ ëª¨ë¸: Apache 2.0 License
- í•™ìŠµ ë°ì´í„°: ë¹„ê³µê°œ (ë¹„ìƒì—…ì  ì—°êµ¬ ëª©ì  ì‚¬ìš© ê°€ëŠ¥)

## ğŸ™‹â€â™€ï¸ ì œì‘ì ì •ë³´

- ì´ë¦„: ë¥˜í˜„ì • (Hienchong)
- Hugging Face: https://huggingface.co/hienchong
- ëª¨ë¸ í˜ì´ì§€: https://huggingface.co/hienchong/Maildan_kobart_v3
