from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments

model_name = "EbanLee/kobart-summary-v3"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

dataset = load_dataset("json", data_files={"train": "path/to/train_summarized_1000.jsonl"})

prompt = (
    "다음 기사 내용을 단순하게 요약하지 말고, 서사 구조와 맥락을 살려 3~5문장으로 풍부하게 요약해줘. "
    "이슈가 발생한 배경과 원인, 참여자들이 주장하는 핵심 내용이 주장에 담긴 사회적 의미나 쟁점이 포함되도록 작성해줘. "
    "감정적이지 않고 객관적인 어조를 유지하되, 논쟁의 구조는 드러나게 써줘.\n\n"
)

def preprocess_function(examples):
    inputs = [prompt + "문장: " + text + "\n\n글:" for text in examples["input"]]
    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding="max_length")

    labels = tokenizer(examples["output"], max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset["train"].map(preprocess_function, batched=True, remove_columns=["input", "output"])

training_args = TrainingArguments(
    output_dir="./kobart_prompt_tuned2",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=3e-2,
    logging_steps=100,
    save_steps=500,
    save_total_limit=2,
    fp16=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

trainer.train()
